%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}\label{intro-ch}

\section{Spark and MapReduce}


New data processing systems such as Spark and MapReduce have been designed to help process the increasing amount of data. 
Instead of relying on just one powerful computer, these systems use many computers due to lower costs, increased scalability, and improved fault tolerance. 
Because these systems are distributed in nature, they have stages (shuffle stages) where they transfer information between computers. 
\section{Shuffle} 

We will use MapReduce to explain the shuffle in more detail, but the main concepts still apply to Spark.

\subsection{Shuffle Introduction}
In the first stage of MapReduce, the map phase, the data is loaded onto different computers and computation is performed on it that results 
in a group of key-value pairs. The final phase of MapReduce,the reduce phase, assumes that all key-value pairs with the same
key are grouped together onto the same machine. We call this property the shuffle guarentee. Thus, the shuffle phase, an intermediate phase that the system handles internally, transfers key-value pairs between machines to satisfy the shuffle guarentee. 


Figure~\ref{fig:shuffle_basic} display the inner workings 
of the shuffle phase in MapReduce. For instance, a programmer may  want to count the 
number of letters in a distributed file. The mappers will each load part of the distributed file and count the number of
letters in their part. However, the systems needs to aggregate the count for each letter and 
thus all the counts for letter a will be sent to worker1, letter b will be sent to worker 2,
letter c will be sent to worker c. These reducers will then promptly aggregate the counts that they receive from the 
mappers.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{./img/shuffle_basic.png}
\caption{Shuffle for Letter Count in MapReduce}
\label{fig:shuffle_basic}
This figures demonstrates a basic shuffle in MapReduce. 
Red arrows indicate the transfer of letter counts of A,blue arrows are used for B,
and black arrows are used for C. 
\end{center}
\end{figure}

Due of the huge amounts of keys, these systems do not transfer data on the granurality of keys.
Instead, they use partitions, which contain key-value pairs with different keys. Programmers can pick different partitioning functions such as hash partitiong and range partitiong to map keys to partitions. Two identical keys are guarenteed to be in the same partition. As long as all the mappers partition their data in the same way and send each partition with the same index to the same reducer, the system satisfies 
the shuffle guarentee. 

\subsection {Shuffle Analysis}

MapReduce is constrained by the slowest worker; therefore, minimizing the latency of the slowest worker should improve imperformance.
Balancing the amount of data sent to each reducer helps achieve this by reducing both network latency and also the execution time for the slowest worker.
Figure~\ref{fig:shuffle_unbalanced}, depicts a shuffle scenario that results in unbalanced paritions. Each mapper partition gets sent to the reducer id equal to the partition id mod the number of reducers. This protocol in theory should result in pretty balanced reducers but is not guarenteed to. As depicted, Reducer 3 receives 
40MB of data but Reducer 1 receives 90MB of data. However, if we knew the size of each partition after the mappers have run,
we could more intelligently balance the reducers. As seen in Figure~\ref{fig:shuffle_balanced}, with the same map output partitions, the system could attain complete balance
of 60MB for each reducer.


 \begin{figure}[h]
\begin{center}
\includegraphics[scale=1.0]{./img/shuffle_unbalanced.png}
\caption{Unbalanced shuffle of partitions} 
\label{fig:shuffle_unbalanced.png}
\end{center}
\end{figure}

 \begin{figure}[h]
\begin{center}
\includegraphics[scale=1.0]{./img/shuffle_balanced.png}
\caption{Balanced shuffle of partitions.}
\label{fig:shuffle_balanced.png}
\end{center}
\end{figure}

\section{Adaptive Scheduling of Joins}\label{intro-ch:eeg-overview}

\subsection{Join Basics}

A common operation in these data processing environments is a join.
A join basically combines two tables by finding intersections between
keys in respective columns. For instance, if we have   
Table\ref{table:join1} and Table\ref{table:join2} that we are trying to join based on the intersection
of key1 and key2, the resulting output is Table\ref{table:join3}   
\begin{table}[h!]
\centering
 \begin{tabular}{|c |c |c |c|}
  \hline
   Key1 & Value1 \\
  \hline
   a & 1 \\
  \hline
   a & 1 \\
  \hline
   b & 3 \\
  \hline
   c & 4 \\
  \hline
\end{tabular}
\caption{Table for Dataset 1}
\label{table:join1}
\end{table}

\begin{table}[h!]
\centering
 \begin{tabular}{|c |c|}
  \hline
   Key2 & Value2 \\
  \hline
   a & 5 \\
  \hline
   c & 7 \\
  \hline
\end{tabular}
\caption{Table for dataset 2}
\label{table:join2}
\end{table}

\begin{table}[h!]
\centering
 \begin{tabular}{|c |c |c|}
  \hline
   Key1 & Value1 & Value2  \\
  \hline
   a & 1 & 5 \\
  \hline
   a & 2 & 5 \\
  \hline
   c & 4 & 7 \\
  \hline
\end{tabular}
\caption{Table of Joined Data}
\label{table:join3}
\end{table}

\subsection{Shuffle Join}
The actual implementation of joins in MapReduce is very similar to the
shuffle scenario presented above. Instead of one dataset participating in the shuffle,
two datasets participate in the shuffle and ensure that their corresponding partitions 
are both sent to the same reducer.  
Figure \ref{fig:shuffle_join} details a shuffle join.
For both datasets, all of the keys that 
mapped to partition 1 were sent to the reducer 1 and this happens respectively for the rest of the partitions.

 \begin{figure}[h]
\begin{center}
\includegraphics[scale=1.0]{./img/shuffle_join.png}
\caption{Typical Shuffle Join}
\label{fig:shuffle_join}
\end{center}
\end{figure}

\subsection {Broadcast Join}
The diagram above  may seem to imply that mappers and reducers
are different machines. However, this distinction is artificial and there are no seperate machines for mappers and reducers. 
Therefore, not all data in the shuffle stage is transferred over the network. In Figure~\ref{fig:shuffle_basic}, if Mapper 1
and Reducer 1 were the same machine, the key value pair A=7 would be read locally and not have to be the sent over the network.

Because transferring data over the network could be a bottleneck, the broadcast join tries to increase the amount of data 
being read locally.For instance, in Figure~\ref{fig:shuffle_join}, dataset 1 is drastically bigger than dataset 2. As seen in Figure~\ref{fig:broadcast_join},
the broadcast join keeps the bigger dataset in place and sends the entirety of dataset 2 to every reducer. Even though all of dataset 1 stays in place, this method satisfies theshuffle party because all partitionsof dataset 2 are sent. The diagram shows that the network traffic is reduced from megabytes to kilobytes.

Broadcast Join is not always the optimal strategy. Because the entirety of the smaller dataset is sent to all partitions, the amount of total computation time increases. Additionally, if the datasets are approximately the same size, network traffic will actually increase. 
Each join strategy is the optimal strategy in different situations. Thus, it becomes imperative
to be able to pick the strategy after the mappers have run and we know the size of the map output files..

 \begin{figure}[h]
\begin{center}
\includegraphics[scale=1.0]{./img/broadcast_join.png}
\caption{Broadcast Join}
\label{fig:broadcast_join}
\end{center}
\end{figure}


