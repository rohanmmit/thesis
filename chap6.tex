\chapter{Discussion}\label{discuss-ch}

\section{Design Challenges}

Working with the EEG data proved to be a challenge in itself. Starting with a
few Matlab processing scripts that work for small datasets, we set to build a
scalable design. These initial programs would read data from an EDF file,
perform the spectrogram calculations and render a static image of 1 to 2 hours
of data. From this, our design separated the workflow into the three layers,
storage, compute and visualization. Converting the implementations first to
Python and then to C++ (see \ref{discuss-ch:abandon}) and implementing a web based
visualization was non-trival since testing the correctness of the algorithms
could not be done until all of the layers were in place. Once an initial
implementation was completed, it was easier to abstract different portions or
change parts of the calculations for more optimized use. \\

\subsection{Storage Layer}

Creating a useful abstraction for the storage layer was essential for
evaluating different datastores and maintaining scalability. The performance of
this layer is likely the most critical for system performance and also the
hardest to profile and analyse. The difficulty arises from the scale of the
datasets when bugs arise only for very large files during testing, files larger
than the disk space on development machines. To verify correctness with
different backends, conversion between the basckend dataformats was crucial.
Each backend could dump their data in a csv format and be imported into a
different store for debugging.

\subsection{Compute Layer}

The main challenge with the compute layer was becoming familiar with the
medical domain and the algorithms required for different computations. As
discussed in \ref{discuss-ch:abandon}, the performance of this layer can greatly
affect the system as a whole since it connects the storage and visualizations
layers to one another. The most import design decision here was the choice of
language, a tradeoff between development ease and performance. Having an
efficient way to extract data from storage, compute on it, and send it to the
client was essential. By choosing C++ over C we were able to take advantage of
the Armadillo \cite{arma} library for linear algebra processing and an open source
websocket library \cite{websocket-server} for sending data over the network.

\subsection{Visualization Layer}

When designing the visualization layer, the main challenge we found was working
with larger datasets in the browser. The network latency for the browser to
receive such a dataset can be prohibitive in itself. The main fact we were able
to take advantage of was that a human would be analyzing the final dataset.
Analysts can view dramatically downsampled data to make the same conclusion and
the size of a visualization is capped by the number of pixels in the client's
screen. This allows us to still give rich representations in our visualizations
but without adding latency to the analysis.

\section{Abandoned Designs}\label{discuss-ch:abandon}

There were two main directions that the project could have taken that were
abandoned. Initially we wanted to explore performing the EEG calculations on a
GPU to reduce the computation overhead. It became clear that I/O was the
primary bottleneck for analysing the data and since the datasets cannot reside
in memory, the overhead of transferring data to the GPU for processing would be
prohibitive. In addition, after implementing an initial implementation in C++,
the performance results were acceptable and allowed us to focus on the storage
and visualization systems. \\

In addition, we had version of the computation algorithms written in Python.
The benefit of a Python implementation is that it allows rapid development and
testing and also reduces the time for configure the input system as it is more
portable. HDF5 provides Python bindings that allow one to access array data,
however we found that the cost of serialization when sending data over the
network dominated the cost of the calculation. We believe that when trying to
serialize the data to send a visualization over the websockets, the data was
being converted into some Python type and causing a massive slowdown. Rather
than rewrite a websocket library, changing the language seemed appropriate.

\section{Lessons Learned}

The lessons here are a reflection of the overall project and what was learned
throughout the development process. The first point would be working with the
open source community. Pinky makes use of several different small open source
projects \cite{edflib} \cite{dropbox-json} \cite{websocket-server}
\cite{tiledb} in each layer (in addition to larger community libraries) and
without these, the project would not have been completed. These projects are
often maintained by individuals as side projects and can contain bugs or be
incomplete. Recognizing this fact early on was crucial to be able to contribute
back to the projects that were used and work with the developer to make the
projects better by submitting patches and helping them pinpoint bugs. \\


Secondly, using the OpenStack infrastructure allowed us to easily allocate new
machines for testing or experimentation. Initially, it was a struggle to setup
the project and compile, however taking the time to automate this greatly
helped to allow collaboration (such as the Visgoth project) and also to quickly
add machines for demos or experimentation. By realizing that having any easy
way to setup the infrastructure was necessary, the time was taken to create
Docker \cite{docker} images of the project so it can easily be setup and used.
Along with this vein the entire project is available on Github
\cite{eeg-toolkit} for use in the hopes that other doctors will make use of the
software.

\section{Future Work}\label{discuss-ch:future-work}

Pinky provides the most basic interaction that an analyst can use to work with
EEG data, namely they can efficiently scroll through patient records on demand.
In order to further the benefit of such analysis, there are a few directions
that future work could take. We sketch potential designs for each for future
projects.

\subsection{Polystore}

Pinky typically deals with the unique patient identifier known as their medical
record number or \c{mrn}. The identifier is used throughout the system as an
id, for example an analyst requests data by giving a patient's \c{mrn}.
Currently the analyst would have to determine the \c{mrn} to use and would then
able to use Pinky. An extension to the system would be to allow analysts to
query patient information to determine which cases to analyze. The current ad
hoc system involves several different storage systems, so this addition would
expand Pinky to have a polystore architecture. Systems using polystore
architectures have been address in \cite{bigdawg}. \\

In order to query patient information Pinky would have to incorporate a
relational store for basic patient metadata and a free text store to keep
doctor reports containing results of different examinations or medications. An
analyst could then query across these stores, ultimately acquiring an \c{mrn}
to query Pinky's storage layer with. The importance of this lies in the ability
to select patients with similar characteristics or treatments and analyse their
data together.

\subsection{Spectrogram Annotations}\label{discuss-ch:annotations}

In addition to viewing the datasets, it would be beneficial for analysts to
also be able to mark noteworthy sections of the spectrogram. This could be for
to make a reference for another analyst to see or to cross reference with other
data, for example the raw EEG signals, to correlate an event with the patient. \\

The implementation would involve selecting the frequencies of a spectrogram for
a given time interval and storing these sections of the matrix.  The analyst
would be able to associated a small amount of free text with the annotation,
such as a note to why the event is interesting. Subsequent viewers would be
able to see this annotation when browsing the spectrogram.

\subsection{Change Point Detection and Clustering}\label{discuss-ch:cpd}

As the corpus of EEG data grows, marking annotations by hand as described in
\ref{discuss-ch:annotations} is rather tedious.  A more automated way would be to scan
try to automatically cut the spectrogram signal into segments, characterized by
changes in average power. Once each segment is generated, one could cluster
these segments and present them to an analyst for classification. During the
patient scan, the majority of the time goes without incident, giving rise to
large periods of time with no activity. By automatically grouping these
sections together, an analyst would be able to quickly pinpoint areas of
interest and mark them. The difficult part of this problem is properly
extracting features from the dataset for classification. \\

\subsection{Distributed Architecture}\label{discuss-ch:dist-arch}

The current architecture allows an analyst to import and view data that can fit
on a single machine. There are two downsides to this, the data corpus will
never entirely fit on a single machine causing costly I/O to transfer files in
for analysis or certain manual steps to modify the infrastructure, for example
swapping out hard disks. In addition, there is no way to easily share
information between other analysts for collaboration. \\

A more general solution would be to host Pinky in a distributed infrastructure
giving analysts access to patient files across as many machines as necessary.
The system is naturally partitionable by patient records, which could simplify
the design which would only require data to be replicated and access parts of
an array over the network. \\

Given the changes proposed in \ref{discuss-ch:annotations} and \ref{discuss-ch:cpd} a
distributed data service, similar to the DataHub\cite{datahub} would be ideal.
This service would allow hospitals and medical research institutions around the
world to host patient data for collaborative analysis, using the Pinky
computation and visualization layers to power the analysis.

