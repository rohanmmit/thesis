\chapter{Discussion}\label{discuss-ch}

\section{Design Challenges}

Working with the EEG data proved to be a challenge in itself. Starting with
some Matlab processing scripts that worked for small datasets, we set out to
build a scalable design. These initial programs would read data from an EDF
file, perform the spectrogram calculations and render a static image of 1 to 2
hours of data. From this, our design separated the workflow into the three
layers, storage, compute and visualization. Converting the implementations
first to Python and then to C++ (see Section~\ref{discuss-ch:abandon}) and
implementing a web based visualization was non-trival since testing the
correctness of the algorithms required all of the layers to be implemented
correctly. While implementing a layer, we had backwards compatible methods of
testing serialized Matlab data. Once we completed an initial implementation, it
was easier to abstract different portions or change parts of the calculations
for more optimized use. \\

\subsection{Storage Layer}

Creating a useful abstraction for the storage layer was essential for
evaluating different datastores and maintaining scalability. The performance of
this layer is likely the most critical for system performance and also the
hardest to profile and analyze. The difficulty arises from the scale of the
datasets. Many bugs appear only for large files during testing, files which are
larger than the disk space on development machines. To verify correctness with
different backends, conversion between the backend data formats was crucial.
Each backend can dump a dataset to a CSV format to import it into a different
store for debugging.

\subsection{Compute Layer}

The main challenge with the compute layer was becoming familiar with the
medical domain and the algorithms required for different computations. As
discussed in Section~\ref{discuss-ch:abandon}, the performance of this layer
can greatly affect the system as a whole since it connects the storage and
visualizations layers to one another. The most import design decision here was
the choice of language, a tradeoff between development ease and performance.
Having an efficient way to extract data from storage, compute on it, and send
it to the client was essential. By choosing C++ over C we were able to take
advantage of the Armadillo \cite{arma} library for linear algebra processing
and an open source websocket library \cite{websocket-server} for sending data
over the network. By choosing C++ over Python, we were able to achieve
sane runtimes.

\subsection{Visualization Layer}

When designing the visualization layer, the main challenge we found was working
with larger datasets in the browser. The network latency of the browser to
receive such a dataset can be prohibitive in itself. The main fact we were able
to take advantage of was that a human would be analyzing the final dataset.
Analysts can view dramatically downsampled data to make the same conclusions.
Because of this, we cap the size of a visualization density by the number of
pixels in the client's screen. This allows us to still give rich
representations in our visualizations but without adding latency to the
analysis.

\section{Abandoned Designs}\label{discuss-ch:abandon}

There were two main directions that the project could have taken that were
abandoned. Initially we wanted to explore performing the EEG calculations on a
GPU to reduce the computation overhead. It became clear that I/O was the
primary bottleneck for analyzing the data and since the datasets cannot reside
in memory, the overhead of transferring data to the GPU for processing would be
prohibitive. In addition, after implementing an initial implementation in C++,
the performance results were acceptable and allowed us to focus on the storage
and visualization systems. \\

In addition, we had version of the computation algorithms written in Python.
The benefit of a Python implementation is that it allows rapid development and
testing and also reduces the time for configure the input system as it is more
portable. HDF5 provides Python bindings that allow one to access array data,
however we found that the cost of serialization when sending data over the
network dominated the cost of the calculation by several orders of magnitude.
We believe that when trying to serialize the data to send a visualization over
the websockets, Python was converting the data to an internal type causing a
massive slowdown. Rather than rewrite a websocket library, changing the
language seemed appropriate.

\section{Lessons Learned}

The lessons here are a reflection of the overall project and throughout the
development process. The first point would be working with the open source
community. Pinky makes use of the following open source projects \cite{edflib}
\cite{dropbox-json} \cite{websocket-server} \cite{tiledb} \cite{happyhttp} in
each layer (in addition to many other larger community libraries) and without
these, the project would not have been completed. These projects are often
maintained by individuals as side projects and can contain bugs or be
incomplete. Recognizing this fact early on was crucial to be able to contribute
back to the projects and work with the developers to make the projects better
by submitting patches and helping them pinpoint bugs. \\


Secondly, using the OpenStack infrastructure allowed us to easily allocate new
machines for testing or experimentation. Initially, it was a struggle to setup
the project and compile, however taking the time to automate this greatly
helped to allow collaboration (such as the Visgoth project) and also to quickly
add machines for demos or experimentation. By realizing that having any easy
way to setup the infrastructure was necessary, we created Docker \cite{docker}
images of the project so it can easily be setup and used.  Along with this vein
the entire project is available on Github \cite{eeg-toolkit} for use in the
hopes that other doctors will make use of the software.

\section{Future Work}\label{discuss-ch:future-work}

Pinky provides the most basic interaction that an analyst can use to work with
EEG data, namely they can efficiently scroll through patient records on demand.
In order to further the benefit of such analysis, we sketch the following
potential designs for future projects.


\subsection{Visgoth}\label{discuss-ch:visgoth-future-work}

Moving forward, the first step will be to rerun the last experiment from
Section~\ref{visgoth-ch:results}, during which we tested the effect of predicting
\emph{downsample} on the consistency of the \emph{latency} experienced by the
client. For this run, the client profiler will have to be modified to send more
up-to-date profile information. Specifically, the profiler should poll the
network more frequently, so that the bandwidth information that Visgoth uses to
predict \emph{downsample} is still relevant. We're optimistic that this will
yield \emph{latency} values close to our target, given the high $R^2$ scores of
our regression models.\\

In future work, we hope to train similar regression models, but with a richer
set of features that includes all of the statistics that we collect during
profiling. Using bandwidth as the only feature produced high scores for our
regression models because bandwidth also happened to be the single largest
bottleneck during profile collection for our example application. However, if
we had collected a more diverse dataset, it's possible that one profile feature
could have overtaken bandwidth as the bottleneck for some domain of inputs.
Such a pattern is impossible to learn if regression models are only trained on
bandwidth.\\

Similarly, fitting the models to a linear basis in this first prototype worked
well with a single bottleneck. However, in order to learn the regression
pattern described above, it will be necessary to use more advanced non-linear
regressor model.\\

Making the regression models dynamic is another possibility for producing
\emph{latency} values closer to the target. To do this, incoming application
requests would also become part of the training or test dataset, and the models
would update incrementally.\\

Visgoth can also be generalized to apply to generic applications that display
large amounts of data to a user. Future work to make this possible would
include creating a small library of statistics important to any application,
such as network bandwidth. Developers could then take the following steps to
integrate Visgoth into their own application:

\begin{itemize}
  \item Install Visgoth profilers on application server and client.
  \item Use the Visgoth \c{statistic} API to record application-specific
    statistics.
  \item Define application-specific data reduction rules (e.g. serve text, but
    not images).
\end{itemize}

All other steps, including profile collection and training of the regression
models, could be automated by the Visgoth system. In this way, Visgoth could
remove much of the work that would have to be done per application and
automatically provide a consistent expeirence across any such application's
clients.

\subsection{Polystore}

Pinky typically deals with the unique patient identifier known as their medical
record number or \c{mrn}. The identifier is used throughout the system as an
id, for example an analyst requests data by giving a patient's \c{mrn}.
Currently, the analyst would have to determine the \c{mrn} to use independently
and would then able to query Pinky. An extension to the system would be to
allow analysts to query patient information to determine which cases to
analyze. The current ad hoc system used at MGH involves using different storage
systems, so this addition would expand Pinky to have a polystore architecture.
BigDawg \cite{bigdawg} addresses systems using polystore architectures. \\

In order to query patient information Pinky would have to incorporate a
relational store for basic patient metadata and a free text store to keep
doctor reports containing results of different examinations or medications. An
analyst could then query across these stores, ultimately acquiring an \c{mrn}
to query Pinky's storage layer with. The importance of this lies in the ability
to select patients with similar characteristics or treatments and analyze their
data together.

\subsection{Spectrogram Annotations}\label{discuss-ch:annotations}

In addition to viewing the datasets, it would be beneficial for analysts to
also be able to mark noteworthy sections of the spectrogram. This could be for
to make a reference for another analyst to see or to cross reference with other
data, for example the raw EEG signals, to correlate an event with the patient. \\

The implementation would involve selecting the frequencies of a spectrogram for
a given time interval and storing these sections of the matrix.  The analyst
would be able to associated a small amount of free text with the annotation,
such as a note to why the event is interesting. Subsequent viewers would be
able to see this annotation when browsing the spectrogram.

\subsection{Change Point Detection and Clustering}\label{discuss-ch:cpd}

As the corpus of EEG data grows, marking annotations by hand as described in
Section~\ref{discuss-ch:annotations} is rather tedious.  A more automated way
would be to scan the dataset and try to automatically cut the spectrogram signal
into segments, characterized by changes in average power using the cumulative
sum algorithm \cite{cumsum}. After generating the segments, one could cluster these
segments and present them to an analyst for classification. During the patient
scan, the majority of the time goes without incident, giving rise to large
periods of time with inactivity. By automatically clustering these sections
together, an analyst would be able to quickly pinpoint areas of interest and
mark them. The difficult part of this problem is properly extracting features
from the dataset for classification. Some schemes using texture analysis for
feature extraction are recommended in \cite{texture-classification1},
\cite{texture-classification2}, and \cite{auto-segment}. \\

\subsection{Distributed Architecture}\label{discuss-ch:dist-arch}

The current architecture allows an analyst to import and view data that can fit
on a single machine's disk. There are two downsides to this, the entire data
corpus will not fit on a single machine, causing costly I/O to
transfer files in for analysis or certain manual steps to modify the
infrastructure, for example swapping out hard disks. In addition, there is no
way to easily share information between other analysts for collaboration. \\

A more general solution would be to host Pinky in a distributed infrastructure
giving analysts access to patient files across as many machines as necessary.
The system is naturally partitionable by patient records, which could simplify
the design which would only require data replication and access parts of
an array over the network. \\

Given the changes proposed in Section~\ref{discuss-ch:annotations} and
Section~\ref{discuss-ch:cpd} a distributed data service, similar to the
DataHub\cite{datahub} would be ideal.  This service would allow hospitals and
medical research institutions around the world to host patient data for
collaborative analysis, using the Pinky computation and visualization layers to
power the analysis.

