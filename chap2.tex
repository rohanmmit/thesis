\chapter{Storage Layer}\label{storage_ch}

Pinky stores both raw patient data (EEG readings) and calculated visualizations
(spectrograms). The raw EEG data is simply a matrix with a column for each
sensor and a data sample read by the sensor at each row. Similarly, the
spectrogram, a range of frequencies across time, can also be stored as a
multidimensional array. Since both raw EEG data and the spectrogram
visualization can be represented as matrices of float values, the system
requires an efficient way to store and query such large multidimensional
arrays.  The storage layer meets this requirement by providing an abstraction
called the \c{StorageBackend} for storing array based data. The abstraction
gives read and write access to 2-dimensional matrices of \c{float} values.
Since efficient querying of the data is required in order to provide the
interactivity that the system demands, we evaluate several different array
based storage systems and compare the tradeoffs of each against a simple
storage system that we implemented.


\section{Design}

The \c{StorageBackend} abstraction provides a thin layer over existing array
based storage systems to provide a consistent API for evaluation across
different systems. The goal is to efficiently expose read/write access to large
array based datasets. A patient EEG scan can last between 24 hours and 7 days
[cite?]. The raw output of voltage readings yield a file from tens to hundreds
of gigabytes in size. We assume that the entire dataset cannot always fit into
the memory of the server, given that it must concurrently respond to many
client queries. Thus, the \c{StorageBackend} should rely on maintaining an
efficient on disk representation. \\

A \c{StorageBackend} implementation must primarily handle three different
workloads. The first is data set ingestion. This is a write heavy workload --
patient data files must be read from disk and stored in the system. Secondly,
spectrogram data is typically written once in a precomputation step and then
saved for later analysis. The saved data is served to an analyst to view,
requiring efficient reads.

\subsection{Ingestion Workload}

Once the patient scan is complete, the raw data file can be ingested into the
system for analysis. The raw data is initially stored in the European Data
Format (EDF)\cite{edf}, a simple binary format for storing multichannel
biological and physical signals. During ingestion, the data is read in chunks
from an input EDF file and converted for storage within the system. This
workload requires the storage system to efficiently handle the addition of
large datasets. We evaluate the performance when ingesting data of several
implementations in \ref{storage_ch:ingestion-exp}.


\subsection{Precomputation Workload}

After a file is ingested into the system, the raw values are read to compute
the spectrogram for the patient. This precomputation step is important for
minimizing the latency when serving the visualization. The precomputation
calculation involves reading columns of the raw data to perform the spectrogram
calculation and writing the spectrogram matrix back to the datastore. The
details of the calculation are described in \ref{ch3:spectrogram}. This
workload requires efficient reading of individual columns in the raw data and,
similarly to the ingestion workload, the ability to write dense matrices
efficiently.

\subsection{Visualization Serving Workload}

The primary workload the storage system must handle is serving chunks of a
calculated visualization. Analysts will request data for a given patient and
time interval and need to be able to quickly have the results rendered. This
workload requires efficient reading of chunks of the matrix.

\section{Implementation}

The storage layer is implemented in C++, providing a common interface to access
array based datasets. A \c{StorageBackend} is created by inheriting from
\c{AbstractStorageBackend} and implementing the methods described in
\ref{storage_ch:api}, using an existing storage system.. This design allows us to
interchange storage systems without affecting other layers within the overall
system. To compare the tradeoffs between different array store systems for each
workload, we implement 3 version of \c{StorageBackend} using two existing
systems, HDF5 \cite{hdf5} and TileDB \cite{tiledb} and our own implementation
as a baseline.

We begin by describing the API in \ref{storage_ch:api} followed by a description of
each implementation and finally discuss some overall optimizations in
\ref{storage_ch:opt}.

\subsection{StorageBackend API}\label{storage_ch:api}

The \c{StorageBackend} API allows creating, reading, and writing arrays. Each
array is accessed by providing a patient's unique medical record number,
\c{mrn}. The API is as follows:

\begin{lstlisting}
    ArrayMetadata get_array_metadata(string mrn);
    void create_array(string mrn, ArrayMetadata* metadata);
    void open_array(string mrn);
    void read_array(string mrn, int ch, int start_offset,
                    int end_offset, float* buf);
    void write_array(string mrn, int ch, int start_offset,
                    int end_offset, float* buf);
    void close_array(string mrn);
\end{lstlisting}

\subsubsection{\c{ArrayMetadata get\_array\_metadata(string mrn)}}
Each array has an associated \c{ArrayMetadata} object which describes the size
of the matrix. Specifically, \c{ArrayMetadata} stores four \c{int} values which
are used in the spectrogram calculation and are sent to the client to prepare
the visualization. The values are as follows:

\begin{lstlisting}
    int fs;
    int nsamples;
    int nrows;
    int ncols;
\end{lstlisting}

\c{fs} is the frequency rate at which the sampling occurred during the EEG
scan. \c{nsamples} is the size of each row in the matrix and \c{nrows} and
\c{ncols} represent the number of rows and columns in the matrix respectively.
Each array based system has a different way of storing metadata values, the
\c{ArrayMetadata} object provides a common interface to access and serialize them.

\subsubsection{\c{void create\_array(string mrn, ArrayMetadata* metadata);}}
The \c{create\_array} function takes as input a patient's medical record
number, \c{mrn} and a pointer to a \c{ArrayMetadata} object. The function is
responsible for defining the array and persisting the \c{ArrayMetadata} to
disk.

\subsubsection{\c{void open\_array(string mrn);}}
The \c{open\_array} function prepares an array for reading or writing. This
function typically caches certain values for more optimized used. This is
discussed further in \ref{storage_ch:opt}.

\subsubsection{\c{void read\_array(string mrn, int ch, int start\_offset, int end\_offset, float* buf);}}
\c{read\_array} takes in a channel (column) to read, \c{ch}, and a
\c{start\_offset} and \c{end\_offset} describing the subset of rows to
retrieve.  When requesting raw EEG data, a single column is passed in. When
requesting a spectrogram, a special \c{ch} value \c{ALL} is given since we want
to retrieve all columns for a given time range. All data values are read into
the provided buffer \c{buf}.

\subsubsection{\c{void write\_array(string mrn, int ch, int start\_offset, int end\_offset, float* buf);}}
Similarly to \c{read\_array}, \c{write\_array} writes the given column \c{ch}
beginning at \c{start\_offset} and ending at \c{end\_offset} with the values
read from the buffer \c{buf}.

\subsubsection{\c{void close\_array(string mrn);}}
\c{close\_array} frees any cached values for the given \c{mrn} and releases
and resources back to the storage system.

\subsection{EDFBackend}

The \c{EDFBackend} is a read-only implementation of the \c{StorageBackend}
interface. This provides easy access for testing other parts of the system and
conversion between backend types. The backend uses an existing EDF library
implementation \cite{edflib} to read given EDF files. This backend is not
included in the valuation since the ingestion cost is zero and the
visualizations are not stored in the EDF format.

\subsection{BinaryBackend}

The \c{BinaryBackend} is inspired by the EDF format representation and is used
as a baseline against other implementations. This is chosen as the baseline
since the storage is designed to meet the workload requirements and is not a
general array storage system. Comparing the performance of other systems to the
\c{BinaryBackend} allows us to see the overhead a more general purpose system
would incur.\\

The on disk representation is very simple. The first byte of the file contains
a \c{uint32\_t}, containing the length of the header information, $n$. The
following $n$ bytes contain a json encoded string with the \c{ArrayMetadata}
information. Following this, the array is written to disk in column major order
for efficient read access. Since the dataset is dense and the number of rows is
known in advance (\c{nsamples}), reading or writing different sections of the
file can be determined by simple offset calculations.\\

When performing multiple I/O operations consecutively, for example, writing a
file out in chunks, this implementation suffers from multiple disk seeks. Each
time a read or write is performed, the file is opened, we seek to the
appropriate location and then we close the file after performing the operation.
To amortize the disk seeks, we can increase the size of the data chunks being
read or written.

\subsection{HDF5Backend}

The \c{HDF5Backend} uses the Hierarchical Data Format (HDF) version 5
\cite{hdf5} to store the data. HDF5 is an open source `technology suite' which
meets the requirements for storage for our system. HDF5 is capable of
supporting very diverse datasets at scale, fitting the array based model of our
EEG data nicely. \\

Integrating HDF5 as a \c{StorageBackend} is very straightforward. Using the
HDF5 C++ bindings, we wrap the library calls to read and write data without API
methods. HDF5 has it's own metadata storage which we utilize for keeping the
\c{ArrayMetadata}. One interesting not is in HDF5 you must specify if the array
will be read or written in chunks so that the system can layout the data in the
chunk sizes your specify. Our initial implementation left this detail out,
costing almost 2x performance.

\subsection{TileDBBackend}

TileDB\cite{tiledb} is an ongoing research project by Intel labs. TileDB
specializes in storing sparse arrays, offering scalable and efficient access to
these datasets. An input dataset can be an arbitrary multidimensional array,
again fitting out model for a storage system. Although our datasets are dense,
we wanted to investigate the performance of TileDB for dense datasets and see
if it can compete with a more mature project such as HDF5.\\

TileDB offers a C api in a shared library which we wrap to create the
\c{TileDBBackend}. At the time of writing the project does not yet support
metadata so we store the \c{ArrayMetadata} as JSON data in a file. When
defining the array a flag is set to order the data in column major order as the
\c{BinaryBackend} does.

\subsection{Optimizations}\label{storage_ch:opt}

When designing the \c{AbstractStorageBackend} performance was an important
factor. Internally we allow child classes to specify a template type \c{T} as
a cache value and also store a mapping from \c{mrn} to \c{ArrayMetadata}. The
goal is the allow each implementation to store some basic information in memory
rather than having to continually fetch from disk. For example, once
\c{open\_array} is called, we cached objects related to the array for future
use until a call to \c{close\_array}. This allows us to keep a consistent
external API and internally manage access to each library efficiently.

\section{Related Work}
TODO

\section{Evaluation}\label{storage_ch:evaluation}

We evaluate the performance of the different \c{StorageBackend} implementations
by testing measuring the time taken to perform file ingestions
\ref{storage_ch:ingestion-exp} and spectrogram precomputation
\ref{storage_ch:precompute-exp}. All of these experiments were run on the CSAIL
OpenStack infrastructure. We used allocated three instances on `isolated'
hosts, where the virtual machine will run using a hardware thread. Each machine
had 4 cores available with 8GB of ram and Intel Xeon 2.27GHz processors. Each
machine had 333GB of disk space available for the calculations. To reduce
variability between experiments, each experiment was run a total of 3 times,
once on each machine and between runs of each experiment we clear the file
system caches.\\

These two use cases were chosen since they are the primary workloads of the
system and we want to understand how the different implementations perform for
different input sizes.\\

The source code for the experiments and necessary running scripts are available
in the project repository \cite{eeg-toolkit}. The repository contains
instructions for building the project and running the experiments.

\subsection{Ingestion Experiment}\label{storage_ch:ingestion-exp}
The ingestion experiment takes an EDF file and converts it to the appropriate
\c{StorageBackend} format. File sizes ranged in powers of 2 from 1GB to 128GB.
The largest EDF file in the corpus was 150GB, and files commonly range from
5GB-20GB. This range of file sizes was chosen to match possible inputs to the
system so we can understand how different implementations scale. In addition to
varying the file size, we also vary how much data is read/written in chunks
from the input file. To reduce system memory consumption we varied chunk sizes
in powers of 2 from 64MB to 128MB.

\subsection{Precompute Experiment}\label{storage_ch:precompute-exp}
The precomputation experiment calculates the spectrogram for a given medical
record number. We use the same file sizes as in the ingestion experiment, using
the ingested data for this calculation of the spectrogram. Similarly to the
ingestion experiment, we vary the chunk sizes for writing the spectrogram back
to the \c{StorageBackend}. Chunk sizes range in powers of 2 from 64MB to 128MB.
