%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Implementation}\label{intro-ch}

\section{Spark}

All of the code was implemented in Spark. Although the code was implemented in Spark, 
it could also be implemented in MapReduce to achieve similar performance improvement.
The  Resilient Distributed Dataset(RDD) is the main interface within Spark.
The RDD can be created from data or from another RDD. The key attributes of an RDD are its inputs,
the number of partitions, and how each of its partitions is computed based on its inputs.

 Profressor Matei Zaharia added code 
that allowed the tracking of sizes of map output files.   

\section{ShuffledRDD}

The RDD we developed we developed is a new version of ShuffledRDD, ShuffledRDD2. 
Its inputs are first a shuffle dependency, which is basically a bunch of map output partitions, and second a number of reducers, which indicates the 
number of partitions for ShuffledRDD@. 
In The regular ShuffledRDD, each of its partition naively requests a segment of map output partitions as depicted in Figure~\ref{fig:shuffle_unbalanced}.
ShuffledRDD2 implements the more complicated scheme seen in Figure~\ref{fig:shuffle_balanced.} 
As this is a proof of concept, each output ShuffledRDD2 partition can only request consecutive map partitions. In other words, it is impossible for a ShuffledRDD2 partition to have map output partitions 1 and 3, without having 2. For this constraint and the given number of partitions , ShuffleRDD2 is guarenteed to produce the most optimally balanced output. 

\section{Joins}

\subsection{ShuffleReader Changes}

As mentioned in the broadcast join section, the bigger RDD must stay in place. The current interface only allows a reducer to request a specific map output partition
 from all of the mappers. For the bigger RDD, we would thus have to request map output partitions from other machines, which defeats the purpose of the broadcast machine. Thus, we added the capability of requesting a specific partition from just one mapper. 

\subsection{ShuffleJoinRDD and BroadcastJoin RDD}

We implement two different type of RDD's, the ShuffleJoinRDD and the
BroadcastJoinRDD. Both of these RDD's take two shuffle dependencies, which remember
are basically the outputs of map stages, partitioned in a certain way. These 
dependencies must be partitioned in the same way. Otherwise, we have no way
of ensuring that two identical keys are in the same partition.  
\\

The ShuffleJoinRDD implementation is very similar to ShuffledRDD. Instead of fetching map output partitions
from just one dependency, it fetches the corresponding map output partitions from both dependencies.   
For instance, ShuffledJoinRDD partition 1 will fetch dataset1 partition 1 and dataset2
partition 1 from all of the workers. Once these partitions are fetched, it create a map with the key value pairs of the smaller partition.
IT iterate through the bigger partition, seeing if there are keys present in this map, and if so, we add this to ourput.\\

The BroadcastJoinRDD implements the broadcast shuffle.
For each BroadcasstJoinRDD partition, it requests one local map output partition from the bigger RDD using the new request capability and  
all of the paritions from the smaller RDD, thus giving us all of the smaller RDD. We then use the same strategy
to actually join the same strategy as the ShuffledJoinRDD to find the intersections.

\subsection{Joins in Spark SQL}

Although the RDD interface is very popular, many programmers and data analysts prefer not to use this interface
and are more familiar with the sql and thus Spark offers a sql like interface. One popular operation within sql is join.
Although the user still writes in sql, Spark still executes the code using RDD's.\\

Because we are not just using the RDD interface and Spark automatically converts the sql query into a query plan,
the implementation is much more complicated. We only implement our optimization for sort merge join.

Although the exact semantics for how a sort merge join can be found here,the sort merge join requires the shuffle property for the two datasets it is joining.
To help achieve this, the sort merge join applies an exchange operator on each of the mapoutputs. These exchange operators produce
ShuffleRowRDDS, which for our purposes are equivalent to ShuffledRDDS. In the next stage, each partition in the first ShuffledRowRDD is compared
to the partition with the same index in the second ShuffledRowRDD. The only difference between this and how the join RDD's work is pretty semantic
in  that instead of one RDD requesting partitions from multiple mapper, two RDD's repartition their data and then are compared partition by partion.
By default, the code performs a shuffle join almost exactly in a manner with how the ShuffleJoinRDD works. One ShuffleRowRDD requests 
the corresponding partitions from its mapoutput just like Figure~\ref{fig:shuffle_unbalanced} and the other ShuffleRowRDD does the exact same but with its dataset.  
However, if only one input RDD is smaller then a user configured threshold, we use the broadcast join optimization. The bigger ShuffledRowRDD will be exactly like its
parent. The other ShuffledRowRDD will have the same number of partitions as the bigger ShuffledRowRDD with each partition containing the entirety of the smaller input RDD. 
The correctness guarentees are the same as for join RDD's.  

